# -*- coding: utf-8 -*-
"""Final NLP A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/108U-OnFFOdO79uIhdhhOw8aYlepYySTy
"""
import nltk
# nltk.download('wordnet')
import numpy as np
import re
import nltk
from sklearn.datasets import load_files
import gensim
from gensim.models import Word2Vec
import warnings
warnings.filterwarnings(action='ignore')
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
nltk.download('punkt')
from nltk import word_tokenize

Data = load_files(r"/content/txt_sentoken", categories=["pos", "neg"])

X, Y = Data.data, Data.target

stemmer = WordNetLemmatizer()


# Function that takes a string and remove special char ..etc
def clean_text(sen):
    # Remove all the special characters
    document = re.sub(r'\W', ' ', str(sen))

    # remove all single characters
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)

    # Remove single characters from the start
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document)

    # Substituting multiple spaces with single space
    document = re.sub(r'\s+', ' ', document, flags=re.I)

    # Removing prefixed 'b'
    document = re.sub(r'^b\s+', '', document)

    # Converting to Lowercase
    document = document.lower()

    # Lemmatization
    document = document.split()

    document = [stemmer.lemmatize(word) for word in document]
    document = ' '.join(document)
    return document


def get_mean_vector(word2vec_model, words):
    words = [word for word in words if word in word2vec_model.wv.vocab]
    if len(words) >= 1:
        return np.mean(word2vec_model[words], axis=0)
    else:
        return []


# apply tf-idf
def Word_embedd_vector(strings):
    documents = []
    for sen in range(0, len(strings)):
        document = clean_text(strings[sen])

        documents.append(document)

    tokenized_sents = [word_tokenize(i) for i in documents]
    model1 = gensim.models.Word2Vec(tokenized_sents, min_count=2, size=100, window=5, sg=1)
    return model1, tokenized_sents


X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
model, tokenized_sents = Word_embedd_vector(X_train)

X_mean_train = []
X_mean_test = []


for review in tokenized_sents:
    vec = get_mean_vector(model, review)
    if len(vec) > 0:
        X_mean_train.append(vec)

documents2 = []
for sen in range(0, len(X_test)):
    document2 = clean_text(X_test[sen])
    documents2.append(document2)

tokenized_sents2 = [word_tokenize(i) for i in documents2]
for review in tokenized_sents2:
    vec = get_mean_vector(model, review)
    if len(vec) > 0:
        X_mean_test.append(vec)

classifier = SGDClassifier().fit(X_mean_train, y_train)
y_pred = classifier.predict(X_mean_test)
from sklearn import svm

# Create a svm Classifier
clf = svm.SVC(kernel='linear')  # Linear Kernel

# Train the model using the training sets
clf.fit(X_mean_train, y_train)

# Predict the response for test dataset
y_pred2 = clf.predict(X_mean_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print('accuracy %s' % accuracy_score(y_pred, y_test))
print('accuracy %s' % accuracy_score(y_pred2, y_test))

def predict(text):
  text = clean_text(text)
  text=word_tokenize(text)
  vec=get_mean_vector(model,text)
  pred = classifier.predict(vec)
  return pred

print('Enter your review:')
review = input()
#0 => bad  1=> good
category= predict(review)
if (category == [0]):
  print ("you probably didn't like this movie")
else:
  print ("you probably liked this movie")